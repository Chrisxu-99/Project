{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##                                                        Final Project\n",
    "  \n",
    "### Project Overview:\n",
    "The purpose of the project is to see whether smalls businesses create more jobs in California. We use QWI (Quarterly Workforce Indicator), published by the US Census Bureau, as the main data source. Multiple linear regression models will be tested to better capture and isolate covariances between regressors.\n",
    "#### (Important variables)\n",
    "The depandent variable in the regression model will be the net job creation. This is calculated by subtracting number of hirings for replacement from the total number of hirings. \n",
    "\n",
    "The independent variable will be the proportion of employment of small firms with respect to the total employment in the county. This variable is to reflect the weighted proportion of small businesses. \n",
    "\n",
    "The first set of controlled variables will include the proportion of employments of medium sized and large firms. \n",
    "\n",
    "The second controlled variable is the education contribution level for each small firms. This is calculated by the number of employments with education level higher than or equal to undergraduate level divided by the total employments of small firms. \n",
    "\n",
    "The third controlled variable will be a dummy variable indicating the financial crisis from 2008 to 2010. \n",
    "### Project Structure:\n",
    "#### (Introduction) \n",
    "The project will start with data description, which will include descriptive statistics for each variable. Also, the data structure of QWI will be briefly introduced since there are randoms distortions added for the confidentiality purpose. It is neccessary to consider this kind of error factors.\n",
    "#### (Methodology) \n",
    "The second part will introduce main data process methodology including processing missing data and processing \"significantly distorted value\". Also, variables from the raw dataset will be processed to better eliminate factors that will affect the robustness of the regression, such as seasonality. After these steps, we can present the final processed dataset with simple descriptive statistics such as mean, variance, correlation and covariance. \n",
    "#### (Results) \n",
    "The third part will be constructing regression models. Both linear and logistic regression models will be tested with different regressor inputs. \n",
    "#### (Discussion) \n",
    "The fourth part will be checking the robustness of different regression models and the significance of important coefficients. Robustness testing will include the discussion of adjusted-R, covariance matrix and BIC so that the most preferred model could be selected. Also, the significance will be tested using t-test on important coefficients. \n",
    "#### (Conclusion) \n",
    "The final part will be interpreting the results in case of real-life suggestions and implications. Sugesstions for future research will be included as well.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1. Introduction\n",
    "Job creation has been a central topic in politics, economics, even on people's dinner tables for a long time. People always want a higher job creation rate. However, defining job creation itself is a complicated task consedering there are continuous job destructions and rehirings happening in the job market. Therefore, a more careful and comprehensive approach is needed to better explain the concept of job creation. On top of that, this research contains statistical analysis on the factors that lead to a higher job creation. The hypotheis is that micro and small firms have a higher contribution to job creation. \n",
    "\n",
    "The dataset is from QWI (Quarterly Workforce Indicators) published by the US Census Bureau. It contains time-series data on the establishment level from 1991 to 2019. The quarterly indicators include job changes on firm levels and on individual levels. One specific thing worths mentioning is that the dataset contains artificially distorted data in order to protect the confidentiality of firms. Also, some data at in early years are missing. Therefore, these two factors will affect the quality of the dataset, and we will process these factors to minimized their impact.\n",
    "\n",
    "We will use the unpooled OLS linear regression model to visualize the contribution of micro and small firms. For the result to be robust, different sets of controlled variables including knowledge contribution, job creation by firms of other sizes, and the factor of major financial crisis or business cycles are included in the regression model. Specific explanations of these important variables will be included in the later sessions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 2. Key Imports and Data Access\n",
    "\n",
    "## 2.1 Key Imports\n",
    "We used some famous Python libraries including NumPy, Pandas, and so on for data analysis, regression analysis and data visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#key imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.stats.api as sms\n",
    "import seaborn as sns\n",
    "\n",
    "from statsmodels.iolib.summary2 import summary_col, summary_params\n",
    "from statsmodels.compat import lzip\n",
    "from statsmodels.graphics.gofplots import ProbPlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#highlight extension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2.2 Data Access\n",
    "We will access multiple datasets since we want to make each step as clear as possible while ensuring the accurateness of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#access data\n",
    "url = 'https://ledextract.ces.census.gov/request/cf8a9e8648c947f199360376d7593e38.csv'\n",
    "raw_data = pd.read_csv(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#access data for calculating knowledge contribution\n",
    "url_2 = 'https://ledextract.ces.census.gov/request/9f742fbec5df4712910f9c3859880803.csv'\n",
    "raw_edu = pd.read_csv(url_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#access data for calculating average change in earnings \n",
    "url_3 = 'https://ledextract.ces.census.gov/request/a6e7d73f2f9c4daa984766f0354ebc0e.csv'\n",
    "raw_earns = pd.read_csv(url_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#access data\n",
    "#raw_data = pd.read_csv('dataset_1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Reformatting and Data Cleaning\n",
    "## 3.1 Missing Values\n",
    "There are many missing values in the dataset as shown by the overview. Therefore, we would like to ignore certain groups that contain a significant amount of missing values (20%). Although imputation is a preferred method, we will simply drop certain groups since it remains consistant across the whole dataset and our sample size will not be affected significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get an overview of the dataset\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_edu.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_earns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we observe that there are several columns that are not out interested variables, such as periodicity, seasonadj.\n",
    "#so we want to keep only those useful columns.\n",
    "raw_data = raw_data[['geography','firmsize','year','quarter','Emp','EmpEnd','sEmp','sEmpEnd']]\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check missing values\n",
    "raw_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the pattern of missing values\n",
    "def pct_na (a):\n",
    "    '''return the percentage of the number of missing values for a specified array'''\n",
    "    \n",
    "    number_of_entris = len(a)\n",
    "    number_of_na = a.isna().sum()\n",
    "    result = number_of_na/number_of_entris\n",
    "    \n",
    "    return result\n",
    "\n",
    "def ms_values (dataset, group_name, var_names):\n",
    "    \"\"\"return a DataFrame containing the percentage of missing values for variables var_names by the group group_name\"\"\"\n",
    "    \n",
    "    #group the DataFrame by group_name\n",
    "    grouped = dataset.groupby([group_name])\n",
    "    \n",
    "    #generate the return object\n",
    "    columns = [group_name] + var_names\n",
    "    ms = np.empty([len(grouped),len(var_names)+1])\n",
    "\n",
    "    #apply pct_na on each group (i.e. each year)\n",
    "    column_index = 0\n",
    "    for i in columns:\n",
    "        row_index = 0\n",
    "        for j in grouped.groups:\n",
    "            if column_index == 0:\n",
    "                ms[row_index,column_index] = j\n",
    "            else:\n",
    "                a = grouped.get_group(j)\n",
    "                ms[row_index,column_index] = pct_na(a[i])\n",
    "            row_index +=1\n",
    "        column_index += 1\n",
    "    \n",
    "    ms = pd.DataFrame(ms,columns=columns)\n",
    "    return ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#then we display missing values by county and year\n",
    "\n",
    "ms_county = ms_values(raw_data,'geography',['Emp','EmpEnd'])\n",
    "ms_year = ms_values(raw_data,'year',['Emp','EmpEnd'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Percentage of missing values by county')\n",
    "ms_county.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Percentage of missing values by year')\n",
    "ms_year.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we display counties and years with more than 20% missing values\n",
    "ms_county.loc[ms_county['Emp']>=0.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms_year.loc[ms_year['Emp']>=0.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#same process for the education data\n",
    "raw_edu = raw_edu[['geography','education','year','quarter','Emp','EmpEnd','sEmp','sEmpEnd']]\n",
    "\n",
    "raw_edu.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms_edu_year = ms_values(raw_edu,'year',['Emp','EmpEnd'])\n",
    "ms_edu_county = ms_values(raw_edu,'geography',['Emp','EmpEnd'])\n",
    "\n",
    "ms_edu_year.loc[(ms_edu_year['Emp'] >= 0.2) | (ms_edu_year['EmpEnd'] >= 0.2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms_edu_county.loc[(ms_edu_county['Emp'] >= 0.2) | (ms_edu_county['EmpEnd'] >= 0.2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#same process for the earnings data\n",
    "raw_earns = raw_earns[['geography','year','quarter','EarnS','sEarnS']]\n",
    "\n",
    "raw_earns.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms_earns_year = ms_values(raw_earns,'year',['EarnS'])\n",
    "ms_earns_county = ms_values(raw_earns,'geography',['EarnS'])\n",
    "\n",
    "ms_earns_year.loc[(ms_earns_year['EarnS'] >= 0.2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms_earns_county.loc[(ms_earns_county['EarnS'] >= 0.2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop these observations\n",
    "raw_data = raw_data.loc[raw_data['year']!=1991]\n",
    "raw_data = raw_data.loc[(raw_data['geography']!=6003) & (raw_data['geography']!=6091)]\n",
    "\n",
    "raw_edu = raw_edu.loc[(raw_edu['year'] != 1991) & (raw_edu['geography'] != 6003) & (raw_edu['geography'] != 6091)]\n",
    "\n",
    "raw_earns = raw_earns.loc[(raw_earns['year'] != 1991) & (raw_earns['year'] != 2020) & (raw_earns['geography'] != 6003) & (raw_earns['geography'] != 6091)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we drop all other obervations with missing values\n",
    "raw_data = raw_data.dropna()\n",
    "raw_edu = raw_edu.dropna()\n",
    "raw_earns = raw_earns.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we check to see if we still have missing values\n",
    "raw_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_edu.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_earns.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Distorted Values\n",
    "As mentioned previously, there are distorted values in the dataset for condifentiality purposes. We follow the similar procedure as we have done for missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the number of distorted values for Emp, EmpEnd in raw_data and raw_edu, and EarnS in raw_earns\n",
    "for i in raw_data, raw_edu:\n",
    "    dis_emp = len(i.loc[i['sEmp'] == 9])\n",
    "    dis_emp_end = len(i.loc[i['sEmpEnd'] == 9])\n",
    "    print(f'Number of distorted values in Emp = {dis_emp}''\\n'f'Number of distorted values in EmpEnd = {dis_emp_end}''\\n')\n",
    "\n",
    "dis_earns = len(raw_earns.loc[raw_earns['sEarnS'] == 9])\n",
    "\n",
    "print(f'Number of distorted values in EarnS = {dis_earns}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we oberved a lot of distorted values in raw_data\n",
    "#so we want to see if some years or some counties have exceptionally more distorted values\n",
    "\n",
    "#we first check for years\n",
    "\n",
    "def dis_values(group_name, val_names, dataset):\n",
    "    \n",
    "    grouped = dataset.groupby([group_name])\n",
    "    columns = []\n",
    "    result = []\n",
    "    \n",
    "    for j in val_names:\n",
    "        \n",
    "        columns.append(j)\n",
    "        array = []\n",
    "        index = []\n",
    "        \n",
    "        for i in grouped.groups:\n",
    "            a = grouped.get_group(i)\n",
    "            array.append(len(a.loc[a[j] == 9])/len(a))\n",
    "            index.append(i)\n",
    "        result.append(array)\n",
    "        \n",
    "    result = np.array(result)\n",
    "    \n",
    "    return pd.DataFrame(result.transpose(),columns = columns, index = index)\n",
    "\n",
    "dis_year = dis_values('year',['sEmp','sEmpEnd'],raw_data)\n",
    "dis_year.loc[(dis_year['sEmp'] >= 0.2) & (dis_year['sEmpEnd'] >= 0.2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#then check for counties\n",
    "dis_county = dis_values('geography',['sEmp','sEmpEnd'],raw_data)\n",
    "dis_county.loc[(dis_county['sEmp'] >= 0.2) & (dis_county['sEmpEnd'] >= 0.2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we drop all those counties and all other rows with distorted values\n",
    "\n",
    "raw_data = raw_data.loc[(raw_data['geography'] != 6043 & 6051 & 6105)&(raw_data['sEmp']!=9)&(raw_data['sEmpEnd']!=9)]\n",
    "raw_edu = raw_edu.loc[(raw_edu['sEmp'] != 9) & (raw_edu['sEmpEnd'] != 9)]\n",
    "raw_earns = raw_earns.loc[raw_earns['sEarnS']!=9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#check if there still are distorted values\n",
    "if (9 in raw_data['sEmp'].values or 9 in raw_data['sEmpEnd'].values \n",
    "    or 9 in raw_edu['sEmp'].values or 9 in raw_data['sEmpEnd'].values \n",
    "    or 9 in raw_earns['sEarnS'].values):\n",
    "    \n",
    "    print('Dataset still has distorted values')\n",
    "    \n",
    "else:\n",
    "    print('No distorted values remaining')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Seasonality\n",
    "We check to see if variables suffer from seasonality since our variable values are quarterly based. If so, we will deal with this situation by getting the yearly avarage for each variable in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format ='retina'\n",
    "#display time series values for a certain county and certain categorical variable value\n",
    "x1 = raw_data.loc[(raw_data['geography'] == 6013) & (raw_data['firmsize'] == 0)]\n",
    "xtick = x1['year']\n",
    "x1 = x1['Emp']\n",
    "\n",
    "x2 = raw_edu.loc[(raw_edu['geography'] == 6013) & (raw_edu['education'] == 'E4')]\n",
    "x2 = x2['Emp']\n",
    "\n",
    "x3 = raw_earns.loc[(raw_earns['geography'] == 6013)]\n",
    "x3 = x3['EarnS']\n",
    "\n",
    "fig, axs = plt.subplots(1,3,figsize = [15,4])\n",
    "\n",
    "axs[0].plot(x1)\n",
    "axs[0].set_xticks([])\n",
    "axs[1].plot(x2) \n",
    "axs[1].set_xticks([])\n",
    "axs[2].plot(x3)\n",
    "axs[2].set_xticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Reformatting\n",
    "\n",
    "The original dataset does not have our interested variables, and the format of the original dataset is not feasible for further analysis. Therefore, we calcualte our interested variables and reformat some variables from long to wide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reformatting \n",
    "\n",
    "#get indexes and columns for the new DataFrame\n",
    "county_index = raw_data.drop_duplicates(subset = ['geography'])['geography']\n",
    "year_index = raw_data.drop_duplicates(subset = ['year'])['year']\n",
    "columns_titles = [\"geography\",\"year\",\"firmsize\",\"quarter\",\"Emp\",\"EmpEnd\"]\n",
    "raw_data=raw_data.reindex(columns=columns_titles)\n",
    "\n",
    "#group by three variables\n",
    "grouped = raw_data.groupby(by=[\"geography\",\"year\",\"firmsize\"])\n",
    "\n",
    "#calculate yearly average net job creation rate and base period proportion of each firm size to avoid seasonality\n",
    "result = []\n",
    "\n",
    "for i in grouped.groups:\n",
    "    \n",
    "    a = grouped.get_group(i) # a is a DataFrame for a specific year\n",
    "    county = a['geography'].values[0]\n",
    "    year = a['year'].values[0]\n",
    "    firm_size = a['firmsize'].values[0]\n",
    "    prop = 0\n",
    "    \n",
    "    if firm_size == 0:  # all firm sizes\n",
    "        total_emp_bgn = a['Emp'].values[0]   # first quarter beginning emp for the whole county\n",
    "        total_emp_end = a['EmpEnd'].values[-1]   # last quarter ending emp for the whole county\n",
    "        job_c = (total_emp_end - total_emp_bgn)/total_emp_bgn   # net job creation rate\n",
    "        \n",
    "    else:   # specific firm size\n",
    "        emp_bgn = a['Emp'].values[0]   # first quarter emp for a specific firm size\n",
    "        emp_end = a['EmpEnd'].values[-1]  #last quarter emp for a specific firm size\n",
    "        \n",
    "        if emp_bgn == 0:\n",
    "            prop = (emp_end - emp_bgn) / 0.00001   # deal with the problem of dividing by zero\n",
    "        else:\n",
    "            prop = (emp_end - emp_bgn) / emp_bgn   # employment change rate \n",
    "    \n",
    "    result.append([county,year,firm_size,prop,job_c])\n",
    "    \n",
    "result = pd.DataFrame(data=result,columns=['county','year','firmsize','prop','net_jc'])\n",
    "\n",
    "result = result.loc[result['firmsize'] !=0]   # drop values for the whole county \n",
    "result = pd.get_dummies(result,columns =['firmsize'])   # turn firm size into dummy variable for the next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_edu.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get knowledge contribution\n",
    "grouped_edu = raw_edu.groupby(by=[\"geography\",\"year\",\"education\"])\n",
    "\n",
    "result_edu = []\n",
    "\n",
    "for i in grouped_edu.groups:\n",
    "    \n",
    "    a = grouped_edu.get_group(i)\n",
    "    county = a['geography'].values[0]\n",
    "    year = a['year'].values[0]\n",
    "    edu = a['education'].values[0]\n",
    "    \n",
    "    if edu == 'E0':     \n",
    "        total_emp_bgn = a['Emp'].values[0]   # first quarter beginning emp for the whole county\n",
    "        total_emp_end = a['EmpEnd'].values[-1]   # last quarter ending emp for the whole county\n",
    "        \n",
    "    elif edu == 'E4':\n",
    "        emp_bgn = a['Emp'].values[0]   # first quarter emp for a specific firm size\n",
    "        emp_end = a['EmpEnd'].values[-1]  #last quarter emp for a specific firm size\n",
    "        \n",
    "        if total_emp_bgn == 0:\n",
    "            klg = (emp_bgn - emp_end) / 0.00001   # deal with the problem of dividing by zero\n",
    "            \n",
    "        else:\n",
    "            klg = (emp_end - emp_bgn) / total_emp_bgn   # employment change rate \n",
    "            \n",
    "        result_edu.append([klg])\n",
    "\n",
    "#set multiindex on df_edu\n",
    "edu_county_index = raw_edu.drop_duplicates(subset = ['geography'])['geography']\n",
    "edu_year_index = raw_edu.drop_duplicates(subset = ['year'])['year']\n",
    "\n",
    "mixid = pd.MultiIndex.from_product([edu_county_index,edu_year_index],names = ['county','year'])\n",
    "\n",
    "df_edu = pd.DataFrame(result_edu,columns = ['klg_contribution'],index=mixid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get average change in earnins\n",
    "grouped_earns = raw_earns.groupby(by = ['geography','year'])\n",
    "\n",
    "result_earns = []\n",
    "\n",
    "for i in grouped_earns.groups:\n",
    "    \n",
    "    a = grouped_earns.get_group(i)\n",
    "    county = a['geography'].values[0]\n",
    "    year = a['year'].values[0]\n",
    "    \n",
    "    earns_bgn = a['EarnS'].values[0]   # first quarter earnings\n",
    "    earns_end = a['EarnS'].values[-1]   # last quarter eanings\n",
    "    \n",
    "    if earns_bgn == 0:\n",
    "        earns = earns_bgn / 0.00001   # deal with the problem of dividing by zero\n",
    "            \n",
    "    else:\n",
    "        earns = (earns_end - earns_bgn) / earns_bgn   # earnings change rate \n",
    "        \n",
    "    result_earns.append([earns])\n",
    "    \n",
    "#set multiindex on df_earns\n",
    "edu_county_index = raw_earns.drop_duplicates(subset = ['geography'])['geography']\n",
    "edu_year_index = raw_earns.drop_duplicates(subset = ['year'])['year']\n",
    "\n",
    "mixid = pd.MultiIndex.from_product([edu_county_index,edu_year_index],names = ['county','year'])\n",
    "\n",
    "df_earns = pd.DataFrame(result_earns,columns = ['delta_earning'],index=mixid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_earns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change variable firmsize from long to wide\n",
    "grouped2 = result.groupby(['county','year'])\n",
    "table = []\n",
    "\n",
    "for i in grouped2.groups:\n",
    "    \n",
    "    row = []\n",
    "    prop_1 = 0\n",
    "    prop_size = []\n",
    "    b = grouped2.get_group(i)\n",
    "    net_jc = b['net_jc'].values[0]\n",
    "    row =[net_jc]\n",
    "    \n",
    "    for j in ['firmsize_1','firmsize_2','firmsize_3','firmsize_4','firmsize_5']:\n",
    "        \n",
    "        firmsize_values = b[j].values\n",
    "        \n",
    "        if np.count_nonzero(firmsize_values) ==1:\n",
    "            prop_1 = b.loc[b[j]==1]['prop'].values[0]\n",
    "            row.append(prop_1)\n",
    "            \n",
    "        else:\n",
    "            row.append(0)\n",
    "            \n",
    "    table.append(row)\n",
    "\n",
    "#we use multiindex to make the DataFrame cleaner\n",
    "mixid = pd.MultiIndex.from_product([county_index,year_index],names = ['county','year'])\n",
    "\n",
    "df = pd.DataFrame(table,columns = ['net_jc','delta_prop_micro','delta_prop_small','delta_prop_medium','delta_prop_m_l','delta_prop_large'],index=mixid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display the DataFrames after cleaning and reformatting\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_edu.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_earns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge df, df_edu, and df_earns\n",
    "merged_edu = df.merge(df_edu, left_index=True, right_index=True)\n",
    "merged = merged_edu.merge(df_earns, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we want to check if merged DataFrame is correct\n",
    "success = 'Correctly merged'\n",
    "\n",
    "for i in merged.index:\n",
    "    \n",
    "    if (merged.loc[i,'klg_contribution'] != df_edu.loc[i,'klg_contribution'] \n",
    "        or merged.loc[i,'delta_earning'] != df_earns.loc[i,'delta_earning']):\n",
    "        success = 'Failed merge'\n",
    "\n",
    "print(success)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add dummy variable 'great_depression' indicating if the employment is suffering from the Great Depression\n",
    "great_depression = []\n",
    "\n",
    "for i in merged_edu.index:\n",
    "    \n",
    "    year = i[1]\n",
    "    \n",
    "    if year == 2007 or year == 2008 or year == 2009:\n",
    "        great_depression.append(1)\n",
    "        \n",
    "    else: \n",
    "        great_depression.append(0)\n",
    "\n",
    "merged['great_depression'] = great_depression \n",
    "\n",
    "#rename the merged DataFrame for convenience\n",
    "df = merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display final result\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Descriptive Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display the general descriptive statictics for each variable\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#display the distribution distogram for each variable\n",
    "fig, axs = plt.subplots(3, 3, constrained_layout = True, figsize= [15,8])\n",
    "\n",
    "fig.suptitle('Distribution Histogram for Each Variable',fontsize = 18)\n",
    "count = 0\n",
    "for i in df.columns:\n",
    "\n",
    "    axs[int(count/3)][int(count%3)].hist(df[i],bins = 40)\n",
    "    axs[int(count/3)][int(count%3)].set_title(f'Histogram of {i}')\n",
    "    \n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format ='retina'\n",
    "#display scatter plots between net_jc and prop of each firm size\n",
    "fig, axs = plt.subplots(1,5, constrained_layout = True,sharey = True, figsize= [15,3])\n",
    "\n",
    "fig.suptitle('Scatter Plots between Net Job Creation Rate and the Proportion Change for Each Firm Size',fontsize = 18)\n",
    "axs[0].set_ylabel('net_jc')\n",
    "\n",
    "count = 0\n",
    "for i in df.columns[1:6]:\n",
    "\n",
    "    axs[count].scatter(df[i],df['net_jc'],s=2)\n",
    "    axs[count].set_xlabel(i)\n",
    "    \n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format ='retina'\n",
    "#display scatter plot between net_jc and knowledge contribution\n",
    "fig, ax = plt.subplots(1,1,figsize = [7,4])\n",
    "\n",
    "ax.scatter(df['net_jc'],df['klg_contribution'],s=4)\n",
    "ax.set_title('Scatter Plot between Net Job Creation Rate and Knowledge Contribution',fontsize = 16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display the correlations between each variable\n",
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display the covariances\n",
    "df.cov()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Regression Analysis\n",
    "In this session, we will use different different OLS regression models to test the significance of coefficients of delta_prop_micro and delta_prop_small. We will include different sets of controlled variables in each model in order to minimize the possibility of omitting important variables. After all models, we will compare the diagonistic parameters to see if including certain set of controlled variable will increase the efficiency of the model and how it impacts the coefficient on each regressor.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Model 1 (Regressors Only)\n",
    "In this model, we will only include the regressors and no other controlled variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up the dependent variable 'net_jc' and two regressors with constant\n",
    "y = df['net_jc']\n",
    "x = df[['delta_prop_micro','delta_prop_small']]\n",
    "x = sm.add_constant(x)\n",
    "\n",
    "#set up the OLS model\n",
    "model = sm.OLS(y,x)\n",
    "\n",
    "result_1 = model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Model 2 (With the First Set of Controlled Variables)\n",
    "In this model, we will include the first set of controlled variables. It includes the change of the employment proportion for other firm sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up the dependent variable 'net_jc' and two regressors with controlled variables and constant\n",
    "y = df['net_jc']\n",
    "x = df[['delta_prop_micro','delta_prop_small','delta_prop_medium','delta_prop_m_l','delta_prop_large']]\n",
    "x = sm.add_constant(x)\n",
    "\n",
    "#set up the OLS model\n",
    "model = sm.OLS(y,x)\n",
    "\n",
    "result_2 = model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Model 3 (With the Knowledge Contribution)\n",
    "In this model, we will include another controlled variable which is the knowledge contribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up the dependent variable 'net_jc' and two regressors with controlled variables and constant\n",
    "y = df['net_jc']\n",
    "x = df[['delta_prop_micro','delta_prop_small','delta_prop_medium','delta_prop_m_l','delta_prop_large','klg_contribution']]\n",
    "x = sm.add_constant(x)\n",
    "\n",
    "#set up the OLS model\n",
    "model = sm.OLS(y,x)\n",
    "\n",
    "result_3 = model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Model 4 (With the Great Depression)\n",
    "In this model, we will include one dummy variable which indicates whether the market is suffering from the Great Depression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up the dependent variable 'net_jc' and two regressors with controlled variables and constant\n",
    "y = df['net_jc']\n",
    "x = df[['delta_prop_micro','delta_prop_small','delta_prop_medium','delta_prop_m_l','delta_prop_large','klg_contribution','great_depression']]\n",
    "x = sm.add_constant(x)\n",
    "\n",
    "#set up the OLS model\n",
    "model = sm.OLS(y,x)\n",
    "\n",
    "result_4 = model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Model 5 (With the Earning Change Rate)\n",
    "In this model, we will include another controlled variable which is the change rate in earnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up the dependent variable 'net_jc' and two regressors with controlled variables and constant\n",
    "y = df['net_jc']\n",
    "x = df[['delta_prop_micro','delta_prop_small','delta_prop_medium','delta_prop_m_l','delta_prop_large','klg_contribution','great_depression','delta_earning']]\n",
    "x = sm.add_constant(x)\n",
    "\n",
    "#set up the OLS model\n",
    "model = sm.OLS(y,x)\n",
    "\n",
    "result_5 = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display the summary for regression models\n",
    "models = [result_1,result_2,result_3,result_4,result_5]\n",
    "model_names = []\n",
    "\n",
    "for i in range(5):\n",
    "\n",
    "    model_names.append(f'Model{i+1}')\n",
    "    \n",
    "regressors = (df.columns[1:].values)\n",
    "regressors = ['const']+(regressors.tolist())\n",
    "\n",
    "\n",
    "summary = summary_col(models, model_names = model_names, stars = True, regressor_order = regressors)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format ='retina'\n",
    "#predicted vs. actual\n",
    "actual = df['net_jc']\n",
    "predicted = result_5.predict()\n",
    "\n",
    "fig, ax = plt.subplots(figsize = [7,5])\n",
    "\n",
    "ax.scatter(actual, predicted, s = 4)\n",
    "ax.set_xlabel('actual value')\n",
    "ax.set_ylabel('predicted value')\n",
    "ax.set_title('Predicted vs. Actual')\n",
    "\n",
    "x = np.linspace(-2,3)/10\n",
    "y = np.linspace(-2,3)/10\n",
    "\n",
    "plt.plot(x,y,'--',c='orange',markersize=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 Regression Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test for linearity\n",
    "name = ['t value', 'p value']\n",
    "test = sms.linear_harvey_collier(result_1)\n",
    "lzip(name, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test for heterosketasticity\n",
    "name = ['Lagrange multiplier statistic', 'p-value',\n",
    "        'f-value', 'f p-value']\n",
    "test = sms.het_breuschpagan(result_5.resid, result_5.model.exog)\n",
    "lzip(name, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test for normality of residuels\n",
    "name = ['Jarque-Bera', 'Chi^2 two-tail prob.', 'Skew', 'Kurtosis']\n",
    "test = sms.jarque_bera(result_5.resid)\n",
    "lzip(test,name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format ='retina'\n",
    "#plot histogram of normalized residuels and scatter plot of normalized residuels vs. fitted values\n",
    "normalized_resid = result_5.get_influence().resid_studentized_internal\n",
    "fig, axs = plt.subplots(1,2,figsize = [14,5])\n",
    "\n",
    "axs[0].hist(normalized_resid, bins = 50)\n",
    "\n",
    "axs[1] = sns.residplot(result_5.fittedvalues, normalized_resid,\n",
    "                          lowess=True,\n",
    "                          scatter_kws={'alpha': 0.5},\n",
    "                          line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format ='retina'\n",
    "#plot quantile-quantile plot\n",
    "model_norm_residuals_abs_sqrt = np.sqrt(np.abs(normalized_resid))\n",
    "\n",
    "QQ = ProbPlot(normalized_resid)\n",
    "plot_lm_2 = QQ.qqplot(line='45', alpha=0.5, color='#4C72B0', lw=1)\n",
    "plot_lm_2.axes[0].set_title('Normal Q-Q')\n",
    "plot_lm_2.axes[0].set_xlabel('Theoretical Quantiles')\n",
    "plot_lm_2.axes[0].set_ylabel('Standardized Residuals');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['net_jc'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std = df['net_jc'].std()\n",
    "mean = df['net_jc'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = df.loc[(df['net_jc'] >= (mean+3*std)) | (df['net_jc'] <= (mean-3*std))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_outlier = df.drop(index = outliers.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_outlier['net_jc'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_no_outlier['net_jc']\n",
    "x = df_no_outlier[['delta_prop_micro','delta_prop_small','delta_prop_medium','delta_prop_m_l','delta_prop_large','klg_contribution','great_depression','delta_earning']]\n",
    "x = sm.add_constant(x)\n",
    "\n",
    "#set up the OLS model\n",
    "model = sm.OLS(y,x)\n",
    "\n",
    "result_6 = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [result_1,result_2,result_3,result_4,result_5,result_6]\n",
    "model_names = []\n",
    "\n",
    "for i in range(6):\n",
    "\n",
    "    model_names.append(f'Model{i+1}')\n",
    "    \n",
    "regressors = (df_no_outlier.columns[1:].values)\n",
    "regressors = ['const']+(regressors.tolist())\n",
    "\n",
    "\n",
    "summary = summary_col(models, model_names = model_names, stars = True, regressor_order = regressors)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format ='retina'\n",
    "#predicted vs. actual\n",
    "actual = df_no_outlier['net_jc']\n",
    "predicted = result_6.predict()\n",
    "\n",
    "fig, ax = plt.subplots(figsize = [7,5])\n",
    "\n",
    "ax.scatter(actual, predicted, s = 4)\n",
    "ax.set_xlabel('actual value')\n",
    "ax.set_ylabel('predicted value')\n",
    "ax.set_title('Predicted vs. Actual')\n",
    "\n",
    "x = np.linspace(-2,3)/10\n",
    "y = np.linspace(-2,3)/10\n",
    "\n",
    "plt.plot(x,y,'--',c='orange',markersize=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = ['Jarque-Bera', 'Chi^2 two-tail prob.', 'Skew', 'Kurtosis']\n",
    "test = sms.jarque_bera(result_6.resid)\n",
    "lzip(test,name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = ['Lagrange multiplier statistic', 'p-value',\n",
    "        'f-value', 'f p-value']\n",
    "test = sms.het_breuschpagan(result_6.resid, result_6.model.exog)\n",
    "lzip(name, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format ='retina'\n",
    "#plot histogram of normalized residuels and scatter plot of normalized residuels vs. fitted values\n",
    "normalized_resid = result_6.get_influence().resid_studentized_internal\n",
    "fig, axs = plt.subplots(1,2,figsize = [14,5])\n",
    "\n",
    "axs[0].hist(normalized_resid, bins = 50)\n",
    "\n",
    "axs[1] = sns.residplot(result_6.fittedvalues, normalized_resid,\n",
    "                          lowess=True,\n",
    "                          scatter_kws={'alpha': 0.5},\n",
    "                          line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_jc = df['net_jc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_jc.sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}